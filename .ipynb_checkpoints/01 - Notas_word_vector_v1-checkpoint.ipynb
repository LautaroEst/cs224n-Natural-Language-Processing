{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vector (word embedding), word2vec y co-ocurrence matrix\n",
    "\n",
    "En estas notas se van a estudiar los temas que involucran word vectors, especialmente los que buscan representar el significado de las palabras a partir de ... (word2vec) y los que buscan ... (co-ocurrence matrix).\n",
    "\n",
    "Estas notas abarcan las lectures 01 y 02 del curso.\n",
    "\n",
    "## Word vector\n",
    "\n",
    "La idea es representar el significado de una palabra con un vector. De esta manera, palabras con significado \"cercano\" (o sea, sinónimos) estarían representadas por vectores cercanos entre ellos en este espacio.\n",
    "\n",
    "## Co-ocurrence matrix (matriz de co-ocurrencia)\n",
    "\n",
    "La primera forma de representar el significado de una palabra es deduciéndolo a partir de su contexto. El razonamiento es: una palabra va a aparecer muy seguido al lado de ciertas palabras, dado que su significado en general se relaciona con estas. Con esto, se puede definir una matriz de coocurrencia:\n",
    "\n",
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the *context window* surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \\dots w_{i-1}$ and $w_{i+1} \\dots w_{i+n}$. We build a *co-occurrence matrix* $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$'s window.\n",
    "\n",
    "**Example: Co-Occurrence with Fixed Window of n=1**:\n",
    "\n",
    "Document 1: \"all that glitters is not gold\"\n",
    "\n",
    "Document 2: \"all is well that ends well\"\n",
    "\n",
    "\n",
    "|     *    | START | all | that | glitters | is   | not  | gold  | well | ends | END |\n",
    "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\n",
    "| START    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\n",
    "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\n",
    "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\n",
    "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\n",
    "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\n",
    "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\n",
    "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\n",
    "| END      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |\n",
    "\n",
    "**Note:** In NLP, we often add START and END tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine START and END tokens encapsulating each document, e.g., \"START All that glitters is not gold END\", and include these tokens in our co-occurrence counts.\n",
    "\n",
    "Más información sobre matrices de co-ocurrencia [acá](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285) y [acá](http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistinctWords(corpus):\n",
    "    \"\"\" \n",
    "        Devuelve una lista con todas las palabras que aparecen en corpus, sin repetición.\n",
    "        Params:\n",
    "            corpus (lista de lista de strings): corpus of todos los documentos\n",
    "        Return:\n",
    "            corpus_words (lista de strings): lista con todas las palabras que aparecen en corpus, \n",
    "            ordenadas alfanuméricamente y sin repetición.\n",
    "            num_corpus_words (entero): tamaño de la lista.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lista con las palabras del corpus sin repetición\n",
    "    corpus_words = sorted(list(set([item for sublist in corpus for item in sublist])))\n",
    "    \n",
    "    # Tamaño de la lista\n",
    "    num_corpus_words = len(corpus_words)\n",
    "\n",
    "    return corpus_words, num_corpus_words\n",
    "\n",
    "\n",
    "def GetCoocurrenceMatrix(corpus, window_size=4):\n",
    "    \"\"\" \n",
    "        Calcula la matriz de co-ocurrencia de un dado corpus, usando la función anterior.\n",
    "        Nota: las ventanas de las palabras en los extremos son más chicas que las de las \n",
    "        del medio, porque no tienen palabras en uno de los lados.\n",
    "    \n",
    "        Params:\n",
    "            corpus (lista de lista de strings): corpus of todos los documentos.\n",
    "            window_size (entero): tamaño de la ventana.\n",
    "            \n",
    "        Return:\n",
    "            M: 2-D numpy array que representa la matriz de co-ocurrencia. El orden en que están las\n",
    "            palabras es el mismo en que está la lista que devuelve la función DistinctWords(corpus).\n",
    "            word2Ind (diccionario): Diccionario para obtener los índices de la matriz M a partir \n",
    "            de la palabra.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtengo las palabras del corpus sin repetición\n",
    "    words, num_words = DistinctWords(corpus)\n",
    "    M = np.zeros((num_words,num_words))\n",
    "    word2Ind = {key: value for (key,value) in zip(words,range(num_words))}\n",
    "    \n",
    "    # Lleno la matriz de co-ocurrencias\n",
    "    for sentence in corpus:\n",
    "        current_index = 0\n",
    "        sentence_len = len(sentence)\n",
    "        indices = [word2Ind[i] for i in sentence]\n",
    "        while current_index < sentence_len:\n",
    "            left  = max(current_index - window_size, 0)\n",
    "            right = min(current_index + window_size + 1, sentence_len) \n",
    "            current_word = sentence[current_index]\n",
    "            current_word_index = word2Ind[current_word]\n",
    "            words_around = indices[left:current_index] + indices[current_index+1:right]\n",
    "            \n",
    "            for ind in words_around:\n",
    "                M[current_word_index, ind] += 1\n",
    "            \n",
    "            current_index += 1\n",
    "                    \n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A esto se suele hacer una reducción de la dimensionalidad con PCA o con SVD. Esto implica achicar la matriz con esa técnica y obtener una con una dimensión mucho más chica.\n",
    "\n",
    "REVISAR EL CONCEPTO DE SVD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
